{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics - Assignment 5 \n",
    "# Topic: Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: NA\n",
    "## Shailesh Sridhar - 01FB16ECS349\n",
    "## Shashank Prabhakar - 01FB16ECS356\n",
    "## Shrey Tiwari - 01FB16ECS368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import operator\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading datasets\n",
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>244</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>254</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>805</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1075</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1131</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1261</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1424</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1435</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1733</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2197</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2295</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2766</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2977</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3346</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3363</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3373</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3467</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4017</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4571</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4785</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4795</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4938</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5048</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5091</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5439</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5476</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5539</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5582</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5741</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>274808</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>275020</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>275210</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>275306</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>275401</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>275571</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>275611</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>275638</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>275677</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>276027</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>276050</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>276165</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>276231</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>276328</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>276681</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>277042</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>277623</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>277639</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>277901</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>277922</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>277923</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>277928</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>277937</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>278026</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>278188</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>278194</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>278356</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>278582</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>278633</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>278854</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id     rating\n",
       "0         243   7.000000\n",
       "1         244   7.000000\n",
       "2         254   7.000000\n",
       "3         805   8.000000\n",
       "4        1075   8.000000\n",
       "5        1131   7.333333\n",
       "6        1261   6.000000\n",
       "7        1424   7.333333\n",
       "8        1435   9.000000\n",
       "9        1733   5.000000\n",
       "10       2197   9.000000\n",
       "11       2295   6.000000\n",
       "12       2766   8.000000\n",
       "13       2977   7.000000\n",
       "14       3346   9.000000\n",
       "15       3363  10.000000\n",
       "16       3373   9.000000\n",
       "17       3467   9.000000\n",
       "18       4017   9.000000\n",
       "19       4571   7.500000\n",
       "20       4785   9.000000\n",
       "21       4795   9.000000\n",
       "22       4938   8.500000\n",
       "23       5048  10.000000\n",
       "24       5091   8.000000\n",
       "25       5439   8.500000\n",
       "26       5476   4.000000\n",
       "27       5539   9.500000\n",
       "28       5582   9.500000\n",
       "29       5741   8.000000\n",
       "...       ...        ...\n",
       "2487   274808   9.000000\n",
       "2488   275020   9.000000\n",
       "2489   275210   8.500000\n",
       "2490   275306  10.000000\n",
       "2491   275401   5.000000\n",
       "2492   275571   8.000000\n",
       "2493   275611   5.000000\n",
       "2494   275638   6.000000\n",
       "2495   275677   8.500000\n",
       "2496   276027   5.000000\n",
       "2497   276050   8.333333\n",
       "2498   276165   9.000000\n",
       "2499   276231   7.000000\n",
       "2500   276328   7.500000\n",
       "2501   276681   8.500000\n",
       "2502   277042   2.000000\n",
       "2503   277623   6.000000\n",
       "2504   277639   5.000000\n",
       "2505   277901   7.333333\n",
       "2506   277922   9.000000\n",
       "2507   277923   9.000000\n",
       "2508   277928   9.000000\n",
       "2509   277937   8.000000\n",
       "2510   278026   8.000000\n",
       "2511   278188   9.000000\n",
       "2512   278194   9.000000\n",
       "2513   278356  10.000000\n",
       "2514   278582   8.500000\n",
       "2515   278633   8.500000\n",
       "2516   278854   7.000000\n",
       "\n",
       "[2517 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part a\n",
    "user_avg = train.groupby('user_id')['rating'].mean().reset_index() \n",
    "#reset_index adds column name as rating\n",
    "user_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>Average_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st to Die: A Novel</td>\n",
       "      <td>7.607843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Heartbreaking Work of Staggering Genius</td>\n",
       "      <td>7.488889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Is for Alibi (Kinsey Millhone Mysteries (Pap...</td>\n",
       "      <td>7.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Lesson Before Dying (Vintage Contemporaries ...</td>\n",
       "      <td>7.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Man Named Dave: A Story of Triumph and Forgi...</td>\n",
       "      <td>7.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A Walk in the Woods: Rediscovering America on ...</td>\n",
       "      <td>8.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A Wrinkle In Time</td>\n",
       "      <td>8.837838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Absolute Power</td>\n",
       "      <td>7.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ahab's Wife: Or, The Star-Gazer: A Novel</td>\n",
       "      <td>7.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>American Gods</td>\n",
       "      <td>7.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Angela's Ashes (MMP) : A Memoir</td>\n",
       "      <td>8.102041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bee Season: A Novel</td>\n",
       "      <td>7.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bel Canto: A Novel</td>\n",
       "      <td>8.164179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Beloved (Plume Contemporary Fiction)</td>\n",
       "      <td>7.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Big Stone Gap: A Novel (Ballantine Reader's Ci...</td>\n",
       "      <td>7.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Black and Blue</td>\n",
       "      <td>7.606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cat &amp;amp; Mouse (Alex Cross Novels)</td>\n",
       "      <td>8.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chicken Soup for the Soul (Chicken Soup for th...</td>\n",
       "      <td>7.512821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cold Mountain : A Novel</td>\n",
       "      <td>7.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Confessions of a Shopaholic (Summer Display Op...</td>\n",
       "      <td>7.790698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Confessions of an Ugly Stepsister : A Novel</td>\n",
       "      <td>7.510204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Cruel &amp;amp; Unusual (Kay Scarpetta Mysteries (...</td>\n",
       "      <td>8.323529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Dance upon the Air (Three Sisters Island Trilogy)</td>\n",
       "      <td>8.119048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Dead Sleep</td>\n",
       "      <td>7.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Death du Jour</td>\n",
       "      <td>8.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Don't Sweat the Small Stuff and It's All Small...</td>\n",
       "      <td>7.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Drowning Ruth (Oprah's Book Club)</td>\n",
       "      <td>7.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Face the Fire (Three Sisters Island Trilogy)</td>\n",
       "      <td>8.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Fall On Your Knees (Oprah #45)</td>\n",
       "      <td>7.566038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fast Food Nation: The Dark Side of the All-Ame...</td>\n",
       "      <td>8.349206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>The Diary of Ellen Rimbauer: My Life at Rose Red</td>\n",
       "      <td>6.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>The Divine Secrets of the Ya-Ya Sisterhood: A ...</td>\n",
       "      <td>7.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>7.813333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>The Hobbit : The Enchanting Prelude to The Lor...</td>\n",
       "      <td>8.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>The Honk and Holler Opening Soon</td>\n",
       "      <td>8.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>The Hours: A Novel</td>\n",
       "      <td>7.765957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>The Last Time They Met : A Novel</td>\n",
       "      <td>6.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>The Lovely Bones: A Novel</td>\n",
       "      <td>8.289683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>The Nanny Diaries: A Novel</td>\n",
       "      <td>7.194030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>The No. 1 Ladies' Detective Agency (Today Show...</td>\n",
       "      <td>8.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>The Pilot's Wife : A Novel</td>\n",
       "      <td>7.781818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>The Poisonwood Bible: A Novel</td>\n",
       "      <td>8.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>The Red Tent (Bestselling Backlist)</td>\n",
       "      <td>8.442857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>The Saving Graces: A Novel</td>\n",
       "      <td>7.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>The Search</td>\n",
       "      <td>7.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>The Tale of the Body Thief (Vampire Chronicles...</td>\n",
       "      <td>7.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>The Vampire Lestat (Vampire Chronicles, Book II)</td>\n",
       "      <td>8.229730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Three To Get Deadly : A Stephanie Plum Novel (...</td>\n",
       "      <td>8.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Tis: A Memoir</td>\n",
       "      <td>7.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Tribulation Force: The Continuing Drama of Tho...</td>\n",
       "      <td>8.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Two for the Dough</td>\n",
       "      <td>8.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Vinegar Hill (Oprah's Book Club (Paperback))</td>\n",
       "      <td>6.552632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>We Were the Mulvaneys</td>\n",
       "      <td>6.912281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Where the Heart Is (Oprah's Book Club (Paperba...</td>\n",
       "      <td>8.204724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>White Oleander : A Novel (Oprah's Book Club)</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Who Moved My Cheese? An Amazing Way to Deal wi...</td>\n",
       "      <td>7.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Wicked: The Life and Times of the Wicked Witch...</td>\n",
       "      <td>7.881579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Wild Animus</td>\n",
       "      <td>3.839080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Year of Wonders</td>\n",
       "      <td>8.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>\\O\\\" Is for Outlaw\"</td>\n",
       "      <td>7.757576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 book  Average_Rating\n",
       "0                                 1st to Die: A Novel        7.607843\n",
       "1           A Heartbreaking Work of Staggering Genius        7.488889\n",
       "2   A Is for Alibi (Kinsey Millhone Mysteries (Pap...        7.560000\n",
       "3   A Lesson Before Dying (Vintage Contemporaries ...        7.411765\n",
       "4   A Man Named Dave: A Story of Triumph and Forgi...        7.885714\n",
       "5   A Walk in the Woods: Rediscovering America on ...        8.531250\n",
       "6                                   A Wrinkle In Time        8.837838\n",
       "7                                      Absolute Power        7.800000\n",
       "8            Ahab's Wife: Or, The Star-Gazer: A Novel        7.656250\n",
       "9                                       American Gods        7.857143\n",
       "10                    Angela's Ashes (MMP) : A Memoir        8.102041\n",
       "11                                Bee Season: A Novel        7.863636\n",
       "12                                 Bel Canto: A Novel        8.164179\n",
       "13               Beloved (Plume Contemporary Fiction)        7.942857\n",
       "14  Big Stone Gap: A Novel (Ballantine Reader's Ci...        7.475000\n",
       "15                                     Black and Blue        7.606061\n",
       "16                Cat &amp; Mouse (Alex Cross Novels)        8.370370\n",
       "17  Chicken Soup for the Soul (Chicken Soup for th...        7.512821\n",
       "18                            Cold Mountain : A Novel        7.520833\n",
       "19  Confessions of a Shopaholic (Summer Display Op...        7.790698\n",
       "20        Confessions of an Ugly Stepsister : A Novel        7.510204\n",
       "21  Cruel &amp; Unusual (Kay Scarpetta Mysteries (...        8.323529\n",
       "22  Dance upon the Air (Three Sisters Island Trilogy)        8.119048\n",
       "23                                         Dead Sleep        7.727273\n",
       "24                                      Death du Jour        8.093750\n",
       "25  Don't Sweat the Small Stuff and It's All Small...        7.800000\n",
       "26                  Drowning Ruth (Oprah's Book Club)        7.476190\n",
       "27       Face the Fire (Three Sisters Island Trilogy)        8.388889\n",
       "28                     Fall On Your Knees (Oprah #45)        7.566038\n",
       "29  Fast Food Nation: The Dark Side of the All-Ame...        8.349206\n",
       "..                                                ...             ...\n",
       "70   The Diary of Ellen Rimbauer: My Life at Rose Red        6.833333\n",
       "71  The Divine Secrets of the Ya-Ya Sisterhood: A ...        7.750000\n",
       "72                                   The Great Gatsby        7.813333\n",
       "73  The Hobbit : The Enchanting Prelude to The Lor...        8.760000\n",
       "74                   The Honk and Holler Opening Soon        8.225000\n",
       "75                                 The Hours: A Novel        7.765957\n",
       "76                   The Last Time They Met : A Novel        6.904762\n",
       "77                          The Lovely Bones: A Novel        8.289683\n",
       "78                         The Nanny Diaries: A Novel        7.194030\n",
       "79  The No. 1 Ladies' Detective Agency (Today Show...        8.153846\n",
       "80                         The Pilot's Wife : A Novel        7.781818\n",
       "81                      The Poisonwood Bible: A Novel        8.269231\n",
       "82                The Red Tent (Bestselling Backlist)        8.442857\n",
       "83                         The Saving Graces: A Novel        7.470588\n",
       "84                                         The Search        7.558140\n",
       "85  The Tale of the Body Thief (Vampire Chronicles...        7.145833\n",
       "86   The Vampire Lestat (Vampire Chronicles, Book II)        8.229730\n",
       "87  Three To Get Deadly : A Stephanie Plum Novel (...        8.250000\n",
       "88                                      Tis: A Memoir        7.600000\n",
       "89  Tribulation Force: The Continuing Drama of Tho...        8.272727\n",
       "90                                  Two for the Dough        8.434783\n",
       "91       Vinegar Hill (Oprah's Book Club (Paperback))        6.552632\n",
       "92                              We Were the Mulvaneys        6.912281\n",
       "93  Where the Heart Is (Oprah's Book Club (Paperba...        8.204724\n",
       "94       White Oleander : A Novel (Oprah's Book Club)        8.000000\n",
       "95  Who Moved My Cheese? An Amazing Way to Deal wi...        7.250000\n",
       "96  Wicked: The Life and Times of the Wicked Witch...        7.881579\n",
       "97                                        Wild Animus        3.839080\n",
       "98                                    Year of Wonders        8.365854\n",
       "99                                \\O\\\" Is for Outlaw\"        7.757576\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part b\n",
    "grouped_items = train.groupby('book')\n",
    "item_avg = grouped_items['rating'].agg(dict(Average_Rating = np.mean)).reset_index()\n",
    "item_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS error without built-in function is: 1.78133640702\n",
      "RMS error with built-in function is: 1.78133640702\n"
     ]
    }
   ],
   "source": [
    "#Part c\n",
    "\n",
    "#Without using built-in function\n",
    "true = train.rating\n",
    "predicted = np.mean(true)\n",
    "pred = []\n",
    "avg = np.mean(true)\n",
    "for i in range(0,len(true)):\n",
    "    pred.append(avg)\n",
    "def rmse(true, predicted):\n",
    "    differences = true - predicted                      \n",
    "    differences_squared = differences ** 2                    \n",
    "    mean_of_differences_squared = differences_squared.mean()  \n",
    "    rmse_val = np.sqrt(mean_of_differences_squared)          \n",
    "    return rmse_val\n",
    "rmse_val = rmse(true, predicted)\n",
    "print(\"RMS error without built-in function is: \" + str(rmse_val))\n",
    "\n",
    "#Using builtin function\n",
    "rms = np.sqrt(mean_squared_error(true, pred))\n",
    "print(\"RMS error with built-in function is: \" + str(rms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   7]\n",
      " [  1   1   8]\n",
      " [  2   2   8]\n",
      " ..., \n",
      " [846  51  10]\n",
      " [847  28   9]\n",
      " [419  46  10]]\n",
      "TEST MATRIX:\n",
      "There are 24 zero entries out of 3666 total elements\n",
      "Sparsity Percentage: 0.6546644844517185\n",
      "[[   0    0    5]\n",
      " [   1    1    9]\n",
      " [   2    2    6]\n",
      " ..., \n",
      " [ 635   73   10]\n",
      " [ 283   41    8]\n",
      " [2516   78    7]]\n",
      "TRAIN MATRIX:\n",
      "There are 254 zero entries out of 16098 total elements\n",
      "Sparsity Percentage: 1.5778357559945337\n"
     ]
    }
   ],
   "source": [
    "#Part d\n",
    "\n",
    "#Test Matrix\n",
    "data1 = test\n",
    "rows = data1.user_id.unique()\n",
    "cols = data1['book'].unique()\n",
    "data1 = data1[['user_id', 'book', 'rating']]\n",
    "idict = dict(zip(cols, range(len(cols))))\n",
    "udict = dict(zip(rows, range(len(rows))))\n",
    "data1.user_id = [udict[i] for i in data1.user_id]\n",
    "data1['book'] = [idict[i] for i in data1['book']]\n",
    "test_matrix = data1.as_matrix()\n",
    "print(test_matrix)\n",
    "#Sparsity of test_matrix\n",
    "nrow1 = test_matrix.shape[0]\n",
    "ncol1 = test_matrix.shape[1]\n",
    "count_zero1 = 0\n",
    "for i in range(0,nrow1):\n",
    "    for j in range(0,ncol1):\n",
    "        if(test_matrix[i][j] == 0):\n",
    "            count_zero1 = count_zero1 + 1\n",
    "\n",
    "sparsity_test = count_zero1/(nrow1 * ncol1)\n",
    "test_sparsity_percentage = sparsity_test*100\n",
    "print(\"TEST MATRIX:\")\n",
    "print(\"There are \"+str(count_zero1)+\" zero entries out of \"+ str(nrow1 * ncol1)+ \" total elements\")\n",
    "print(\"Sparsity Percentage: \" + str(test_sparsity_percentage))\n",
    "    \n",
    "\n",
    "#Train Matrix\n",
    "data2 = train\n",
    "rows = data2.user_id.unique()\n",
    "cols = data2['book'].unique()\n",
    "data2 = data2[['user_id', 'book', 'rating']]\n",
    "idict = dict(zip(cols, range(len(cols))))\n",
    "udict = dict(zip(rows, range(len(rows))))\n",
    "data2.user_id = [udict[i] for i in data2.user_id]\n",
    "data2['book'] = [idict[i] for i in data2['book']]\n",
    "train_matrix = data2.as_matrix()\n",
    "print(train_matrix)\n",
    "#Sparsity of train_matrix\n",
    "nrow2 = train_matrix.shape[0]\n",
    "ncol2 = train_matrix.shape[1]\n",
    "count_zero2 = 0\n",
    "for k in range(0,nrow2):\n",
    "    for l in range(0,ncol2):\n",
    "        if(train_matrix[k][l] == 0):\n",
    "            count_zero2 = count_zero2 + 1\n",
    "\n",
    "sparsity_train = count_zero2/(nrow2 * ncol2)\n",
    "train_sparsity_percentage = sparsity_train*100\n",
    "print(\"TRAIN MATRIX:\")\n",
    "print(\"There are \"+str(count_zero2)+\" zero entries out of \"+ str(nrow2 * ncol2)+ \" total elements\")\n",
    "print(\"Sparsity Percentage: \" + str(train_sparsity_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS Error:  2.28475661194\n"
     ]
    }
   ],
   "source": [
    "#Part e\n",
    "def predict_naive(user, item):\n",
    "    prediction = imean1[item] + umean1[user] - amean1\n",
    "    return prediction\n",
    "naive = np.zeros((len(rows),len(cols)))\n",
    "for row in train_matrix:\n",
    "    naive[row[0], row[1]] = row[2]\n",
    "\n",
    "amean1 = np.mean(naive[naive!=0])\n",
    "nnaiverows = naive.shape[0]\n",
    "nnaivecols = naive.shape[1]\n",
    "\n",
    "naive = np.zeros((len(rows),len(cols)))\n",
    "for row in train_matrix:\n",
    "    naive[row[0], row[1]] = row[2]\n",
    "amean1 = np.mean(naive[naive!=0])\n",
    "umean1 = sum(naive.T) / sum((naive!=0).T)\n",
    "imean1 = sum(naive) / sum((naive!=0))          \n",
    "            \n",
    "predictions = []\n",
    "targets = []\n",
    "for row in test_matrix:\n",
    "    user, item, actual = row[0], row[1], row[2]\n",
    "    predictions.append(predict_naive(user, item))\n",
    "    targets.append(actual)\n",
    "targets = np.array(targets)\n",
    "predictions = np.array(predictions)\n",
    "print(\"RMS Error: \",rmse(targets,predictions)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Part a\n",
    "\n",
    "def cos(mat, a, b):\n",
    "    if a == b:\n",
    "        return 1\n",
    "    aval = mat.T[a].nonzero()\n",
    "    bval = mat.T[b].nonzero()\n",
    "    corated = np.intersect1d(aval, bval)\n",
    "    if len(corated) == 0:\n",
    "        return 0\n",
    "    avec = np.take(mat.T[a], corated)\n",
    "    bvec = np.take(mat.T[b], corated)\n",
    "    val = 1 - cosine(avec, bvec)\n",
    "    if np.isnan(val):\n",
    "        return 0\n",
    "    return val\n",
    "\n",
    "def pr(mat, a, b, imean):\n",
    "    if a == b:\n",
    "        return 1\n",
    "    aval = mat.T[a].nonzero()\n",
    "    bval = mat.T[b].nonzero()\n",
    "    corated = np.intersect1d(aval, bval)\n",
    "    if len(corated) < 2:\n",
    "        return 0\n",
    "    avec = np.take(mat.T[a], corated)\n",
    "    bvec = np.take(mat.T[b], corated)\n",
    "    avec1 = avec - imean[a]\n",
    "    bvec1 = bvec - imean[b]\n",
    "    val = 1 - cosine(avec1, bvec1)\n",
    "    if np.isnan(val):\n",
    "        return 0\n",
    "    return val\n",
    "\n",
    "def itemsimilar(mat, option):\n",
    "    # *Calculate amean, umean and imean as before\n",
    "    amean = np.mean(mat[mat!=0])\n",
    "    nmatrows = mat.shape[0]\n",
    "    nmatcols = mat.shape[1]\n",
    "    imean = [0 for i in range(nmatcols)]\n",
    "    umean = [0 for i in range(nmatrows)]\n",
    "    \n",
    "    \n",
    "    for i in range(nmatcols):\n",
    "        imean[i] = sum(mat.T[i]) / sum((mat.T[i]!=0))          \n",
    "    for j in range(nmatrows):\n",
    "        umean[j] = sum(mat[j]) / sum((mat[j]!=0))\n",
    "   \n",
    "    n = mat.shape[1]\n",
    "    # *initialize a zero matrix with dimensions n,n to get the similarity matrix\n",
    "    sim_mat = np.zeros((n,n))\n",
    "    if option == 'pr':\n",
    "    #print(\"PR\")\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                sim_mat[i][j] = pr(mat, i, j, imean)\n",
    "        sim_mat = (sim_mat + 1)/2\n",
    "    elif option == 'cos':\n",
    "    #print(\"COS\")\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                sim_mat[i][j] = cos(mat, i, j)\n",
    "    else:\n",
    "    #print(\"Default\")\n",
    "        sim_mat = cosine_similarity(mat.T)\n",
    "    return sim_mat, amean, umean, imean\n",
    "\n",
    "\n",
    "\n",
    "def kMostSimilar(sim_mat,x,k):\n",
    "    global udict\n",
    "    reverse_dict = sorted(udict.items(),  key=operator.itemgetter(1))\n",
    "    N = k\n",
    "    a = copy.deepcopy(sim_mat)\n",
    "    user_vals = a[udict[x]]\n",
    "    idx_1d = user_vals.argsort()[-N:]\n",
    "    for i in idx_1d:\n",
    "        print(user_vals[i],i,reverse_dict[i][0])\n",
    "\n",
    "\n",
    "rows = data1.user_id.unique()\n",
    "cols = data1['book'].unique()\n",
    "mat = np.zeros((len(rows),len(cols)))\n",
    "for row in test_matrix:\n",
    "    mat[row[0], row[1]] = row[2]\n",
    "cosine_similarity_mat,amean_cos,umean_cos,imean_cos = itemsimilar(mat.T,'cos')\n",
    "pearson_similarity_mat,amean_pearson,umean_pearson,imean_pearson = itemsimilar(mat.T,'pr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Part b                   \n",
    "def predict(user, item, mat, item_similarity, amean, umean, imean,\n",
    "k=20):\n",
    "    nzero = mat[user].nonzero()[0]\n",
    "    if len(nzero) == 0:\n",
    "        return amean\n",
    "    baseline = imean + umean[user] - amean\n",
    "    choice = nzero[item_similarity[item, nzero].argsort()[::-1][:k]]\n",
    "    prediction = ((mat[user, choice] - baseline[choice]).dot(item_similarity[item,\n",
    "    choice])/ sum(item_similarity[item, choice])) + baseline[item]\n",
    "    if np.isnan(prediction):\n",
    "        prediction = amean\n",
    "    if prediction > 10:\n",
    "        prediction = 10\n",
    "    if prediction < 1:\n",
    "        prediction = 1\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error\n",
      "RMSE : 0.7308\n",
      "Test Error\n",
      "RMSE : 2.2465\n"
     ]
    }
   ],
   "source": [
    "#Part c\n",
    "\n",
    "def get_results(train_data, test_data,option, rows, cols, k):\n",
    "# *initialize a zero matrix called full_mat with dim(rows,cols)\n",
    "    full_mat = np.zeros((rows,cols))\n",
    "    for row in train_data:\n",
    "        full_mat[row[0], row[1]] = row[2]\n",
    "    item_similarity, amean, umean, imean = itemsimilar(full_mat, option)\n",
    "    preds = []\n",
    "    real = []\n",
    "    for row in train_data:\n",
    "        preds.append(predict(row[0],row[1],full_mat,item_similarity,amean,umean,imean))\n",
    "        real.append(row[2])\n",
    "    # *initialize empty preds and real lists.\n",
    "    # *For each row in train_data, call the predict function using the above\n",
    "    #values as parameters, as explained before for the naïve prediction. Keep updating\n",
    "    #the preds and real lists.\n",
    "    # *Calculate the RMSE error of both lists.\n",
    "    err1 = rmse(np.array(preds),np.array(real))\n",
    "    print('Train Error')\n",
    "    print('RMSE : %.4f' % err1)\n",
    "    preds = []\n",
    "    real = []\n",
    "    for row in test_data:\n",
    "        preds.append(predict(row[0],row[1],full_mat,item_similarity,amean,umean,imean))\n",
    "        real.append(row[2])\n",
    "    # *Reinitialize the preds and real lists to empty lists\n",
    "    # *For each row in the test_data, call the predict function using the above\n",
    "    #values as parameters, as explained before for the naïve prediction. Keep updating\n",
    "    #the preds and real lists.\n",
    "    # *Calculate the RMSE error of both lists.\\\n",
    "    err2 = rmse(np.array(preds),np.array(real))\n",
    "    print('Test Error')\n",
    "    print('RMSE : %.4f' % err2)\n",
    "get_results(train_matrix,test_matrix,'pr',nnaiverows,nnaivecols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Part d\n",
    "'''The value of k is non-parametric and a general rule of thumb in choosing the value of k is k = sqrt(N)/2, \n",
    "where N stands for the number of samples in your training dataset.A small value of k means that noise will have a higher influence\n",
    "on the result and a large value make it computationally expensive. Generally, it is suggested that we try and keep\n",
    "the value of k odd(in the case of two classes), so that there is no tie between choosing a class. Similarly, \n",
    "in case of 'c' classes, we would avoid choosing k as a multiple of 'c'.\n",
    "\n",
    "A small value of k means that noise will have a higher influence on the result and a large value make it computationally expensive.\n",
    "The class boundaries become smoother with an increase in the value of K.\n",
    "\n",
    "At K=1, we were overfitting the boundaries. Hence, error rate initially decreases and reaches a minima. \n",
    "After the minima point, it then increases with increasing K. To get the optimal value of K, you can segregate the\n",
    "training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K. \n",
    "This value of K should be used for all predictions.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
